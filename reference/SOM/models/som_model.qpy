#!/usr/bin/env python
# -*- coding: utf-8 -*-
# QPY script for SOM model module

'''
SOM - Self-Organizing Map量子模型
基于量子编码和自组织映射的社交网络分析模型
'''

import os
import json
import time
import logging
import numpy as np
import datetime
from typing import Dict, List, Tuple, Any, Optional, Union
from collections import defaultdict

# 量子库导入
import qiskit
from qiskit import QuantumCircuit, QuantumRegister, ClassicalRegister
from qiskit.quantum_info import Statevector
from qiskit import Aer, execute

# 目录常量
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
LOG_DIR = os.path.join(ROOT_DIR, 'logs')
MODEL_DIR = os.path.join(ROOT_DIR, 'data', 'models')

# 关系类型常量
RELATION_TYPES = {
    'FRIEND': 1,
    'FAMILY': 2,
    'COLLEAGUE': 3,
    'ACQUAINTANCE': 4,
    'ROMANTIC': 5
}

# 消息类型常量
MESSAGE_TYPES = {
    'TEXT': 1,
    'IMAGE': 2,
    'AUDIO': 3,
    'VIDEO': 4,
    'LINK': 5
}

# 情绪类型常量
EMOTION_TYPES = {
    'HAPPY': 1,
    'SAD': 2,
    'ANGRY': 3,
    'SURPRISED': 4,
    'NEUTRAL': 5,
    'ANXIOUS': 6,
    'EXCITED': 7
}

# 创建日志和模型目录
os.makedirs(LOG_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# 配置日志
logging.basicConfig(
    filename=os.path.join(LOG_DIR, f'som_model_{datetime.datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('SOMModel')

class QuantumNode:
    """量子节点类，代表自组织映射中的单个节点"""
    
    def __init__(self, node_id: str, dimension: int = 8):
        """
        初始化量子节点
        
        Args:
            node_id: 节点的唯一标识符
            dimension: 量子权重向量的维度
        """
        self.id = node_id
        self.dimension = dimension
        self.weights = np.random.uniform(0, 1, dimension)
        self.connections = {}  # 连接到其他节点的权重
        self.data = {}  # 存储节点相关的额外数据
        
        # 创建并初始化量子态
        self._initialize_quantum_state()
        
    def _initialize_quantum_state(self):
        """初始化节点的量子态"""
        qreg = QuantumRegister(self.dimension, 'q')
        creg = ClassicalRegister(self.dimension, 'c')
        self.circuit = QuantumCircuit(qreg, creg)
        
        # 用权重值初始化量子态
        for i in range(self.dimension):
            theta = np.pi * self.weights[i]
            self.circuit.ry(theta, qreg[i])
            
        # 添加纠缠以增强节点内部特征关联
        for i in range(self.dimension-1):
            self.circuit.cx(qreg[i], qreg[i+1])
    
    def update_weights(self, input_vector: np.ndarray, learning_rate: float):
        """
        更新节点的权重向量
        
        Args:
            input_vector: 输入向量
            learning_rate: 学习率
        """
        self.weights += learning_rate * (input_vector - self.weights)
        self._initialize_quantum_state()  # 重新初始化量子态以反映新权重
    
    def measure_similarity(self, input_vector: np.ndarray) -> float:
        """
        测量节点与输入向量的相似度
        
        Args:
            input_vector: 输入向量
            
        Returns:
            相似度分数
        """
        # 量子方式测量相似度 - 返回欧氏距离
        return np.linalg.norm(self.weights - input_vector)
    
    def add_connection(self, node_id: str, weight: float = 1.0):
        """
        添加到另一个节点的连接
        
        Args:
            node_id: 目标节点ID
            weight: 连接权重
        """
        self.connections[node_id] = weight
        
    def to_dict(self) -> Dict[str, Any]:
        """
        将节点转换为字典表示
        
        Returns:
            节点的字典表示
        """
        return {
            'id': self.id,
            'weights': self.weights.tolist(),
            'connections': self.connections,
            'data': self.data
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'QuantumNode':
        """
        从字典创建节点
        
        Args:
            data: 节点的字典表示
            
        Returns:
            创建的量子节点实例
        """
        node = cls(data['id'], len(data['weights']))
        node.weights = np.array(data['weights'])
        node.connections = data['connections']
        node.data = data['data']
        node._initialize_quantum_state()
        return node

class SOMModel:
    """自组织映射量子模型类"""
    
    def __init__(self, grid_size: Tuple[int, int] = (10, 10), input_dim: int = 8):
        """
        初始化SOM模型
        
        Args:
            grid_size: SOM网格大小 (宽, 高)
            input_dim: 输入特征维度
        """
        self.grid_size = grid_size
        self.input_dim = input_dim
        self.nodes = {}
        self.learning_rate = 0.1
        self.radius = max(grid_size) / 2
        self.time_constant = 1000 / np.log(self.radius)
        
        logger.info(f"初始化SOM模型: 网格大小={grid_size}, 输入维度={input_dim}")
        # 初始化SOM网格
        self._initialize_grid()
        
    def _initialize_grid(self):
        """初始化SOM网格节点"""
        width, height = self.grid_size
        for x in range(width):
            for y in range(height):
                node_id = f"node_{x}_{y}"
                self.nodes[node_id] = QuantumNode(node_id, self.input_dim)
                self.nodes[node_id].data['position'] = (x, y)
    
    def _find_bmu(self, input_vector: np.ndarray) -> str:
        """
        找到最佳匹配单元(Best Matching Unit)
        
        Args:
            input_vector: 输入向量
            
        Returns:
            最佳匹配单元的ID
        """
        min_dist = float('inf')
        bmu_id = None
        
        for node_id, node in self.nodes.items():
            dist = node.measure_similarity(input_vector)
            if dist < min_dist:
                min_dist = dist
                bmu_id = node_id
                
        return bmu_id
    
    def _calculate_influence(self, node_pos: Tuple[int, int], bmu_pos: Tuple[int, int], iteration: int) -> float:
        """
        计算节点受BMU的影响程度
        
        Args:
            node_pos: 节点位置
            bmu_pos: BMU位置
            iteration: 当前迭代次数
            
        Returns:
            影响值
        """
        # 计算距离
        dist = np.linalg.norm(np.array(node_pos) - np.array(bmu_pos))
        
        # 计算当前迭代的半径
        radius = self.radius * np.exp(-iteration / self.time_constant)
        
        # 计算影响值
        if dist <= radius:
            return np.exp(-(dist**2) / (2 * (radius**2)))
        else:
            return 0
            
    def train(self, data: List[np.ndarray], iterations: int = 1000):
        """
        训练SOM模型
        
        Args:
            data: 训练数据列表
            iterations: 训练迭代次数
        """
        logger.info(f"开始训练SOM模型: 数据量={len(data)}, 迭代次数={iterations}")
        
        initial_learning_rate = self.learning_rate
        
        for i in range(iterations):
            # 更新学习率
            self.learning_rate = initial_learning_rate * np.exp(-i / iterations)
            
            # 随机选择一个输入向量
            input_vector = data[np.random.randint(0, len(data))]
            
            # 找到最佳匹配单元
            bmu_id = self._find_bmu(input_vector)
            bmu_pos = self.nodes[bmu_id].data['position']
            
            # 更新所有节点
            for node_id, node in self.nodes.items():
                node_pos = node.data['position']
                influence = self._calculate_influence(node_pos, bmu_pos, i)
                
                if influence > 0:
                    # 应用影响系数更新权重
                    node.update_weights(input_vector, self.learning_rate * influence)
            
            if i % 100 == 0:
                logger.info(f"训练进度: {i}/{iterations}, 学习率: {self.learning_rate:.4f}")
    
    def map_input(self, input_vector: np.ndarray) -> Tuple[str, Dict[str, float]]:
        """
        将输入映射到SOM空间
        
        Args:
            input_vector: 输入向量
            
        Returns:
            (最佳匹配单元ID, 与所有节点的相似度字典)
        """
        similarities = {}
        for node_id, node in self.nodes.items():
            similarities[node_id] = 1.0 / (1.0 + node.measure_similarity(input_vector))
            
        # 找到最佳匹配单元
        bmu_id = max(similarities, key=similarities.get)
        
        return bmu_id, similarities
    
    def cluster_data(self, data: List[np.ndarray]) -> Dict[str, List[int]]:
        """
        将数据聚类
        
        Args:
            data: 要聚类的数据
            
        Returns:
            聚类结果，格式为 {node_id: [data_indices]}
        """
        clusters = defaultdict(list)
        
        for i, vector in enumerate(data):
            bmu_id, _ = self.map_input(vector)
            clusters[bmu_id].append(i)
            
        return dict(clusters)
    
    def analyze_relationships(self, user_features: Dict[str, np.ndarray]) -> Dict[str, Dict[str, float]]:
        """
        分析用户关系
        
        Args:
            user_features: 用户特征字典 {user_id: feature_vector}
            
        Returns:
            用户关系强度矩阵 {user1: {user2: strength}}
        """
        relationships = defaultdict(dict)
        
        # 将每个用户映射到网格
        user_mappings = {}
        for user_id, features in user_features.items():
            bmu_id, similarities = self.map_input(features)
            user_mappings[user_id] = (bmu_id, similarities)
        
        # 计算用户之间的关系强度
        for user1, (bmu1, sim1) in user_mappings.items():
            for user2, (bmu2, sim2) in user_mappings.items():
                if user1 != user2:
                    # 计算两个BMU之间的相似度
                    pos1 = self.nodes[bmu1].data['position']
                    pos2 = self.nodes[bmu2].data['position']
                    distance = np.linalg.norm(np.array(pos1) - np.array(pos2))
                    
                    # 关系强度基于相似度和距离
                    strength = np.exp(-distance / 10.0)  # 标准化距离
                    relationships[user1][user2] = strength
        
        return dict(relationships)
    
    def recommend_connections(self, 
                             user_id: str, 
                             user_features: Dict[str, np.ndarray], 
                             existing_connections: Dict[str, List[str]],
                             top_n: int = 5) -> List[Tuple[str, float]]:
        """
        为用户推荐新连接
        
        Args:
            user_id: 用户ID
            user_features: 所有用户的特征向量
            existing_connections: 现有连接 {user_id: [connected_user_ids]}
            top_n: 返回的推荐数量
            
        Returns:
            推荐的连接列表 [(user_id, score)]
        """
        if user_id not in user_features:
            return []
            
        # 获取用户的现有连接
        user_connections = existing_connections.get(user_id, [])
        
        # 计算关系强度
        relationships = self.analyze_relationships(user_features)
        
        # 过滤掉已存在的连接
        recommendations = []
        for other_user, strength in relationships.get(user_id, {}).items():
            if other_user not in user_connections and other_user != user_id:
                recommendations.append((other_user, strength))
        
        # 按关系强度排序并返回前N个
        recommendations.sort(key=lambda x: x[1], reverse=True)
        return recommendations[:top_n]
    
    def visualize_network(self, 
                          user_features: Dict[str, np.ndarray], 
                          connections: Dict[str, List[str]]) -> Dict[str, Any]:
        """
        生成网络可视化数据
        
        Args:
            user_features: 用户特征
            connections: 用户连接
            
        Returns:
            可视化数据
        """
        visualization = {
            'nodes': [],
            'links': []
        }
        
        # 创建节点
        for user_id, features in user_features.items():
            bmu_id, _ = self.map_input(features)
            position = self.nodes[bmu_id].data['position']
            
            visualization['nodes'].append({
                'id': user_id,
                'group': hash(bmu_id) % 10,  # 简单的分组方法
                'x': position[0],
                'y': position[1]
            })
        
        # 创建连接
        for source, targets in connections.items():
            for target in targets:
                if source in user_features and target in user_features:
                    visualization['links'].append({
                        'source': source,
                        'target': target,
                        'value': 1
                    })
        
        return visualization
    
    def save(self, filepath: Optional[str] = None) -> str:
        """
        保存模型到文件
        
        Args:
            filepath: 保存路径，如果为None则自动生成
            
        Returns:
            保存的文件路径
        """
        if filepath is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filepath = os.path.join(MODEL_DIR, f'som_model_{timestamp}.json')
            
        # 准备保存数据
        model_data = {
            'grid_size': self.grid_size,
            'input_dim': self.input_dim,
            'learning_rate': self.learning_rate,
            'radius': self.radius,
            'time_constant': self.time_constant,
            'nodes': {node_id: node.to_dict() for node_id, node in self.nodes.items()}
        }
        
        with open(filepath, 'w') as f:
            json.dump(model_data, f, indent=2)
            
        logger.info(f"模型保存到: {filepath}")
        return filepath
    
    @classmethod
    def load(cls, filepath: str) -> 'SOMModel':
        """
        从文件加载模型
        
        Args:
            filepath: 模型文件路径
            
        Returns:
            加载的SOM模型
        """
        logger.info(f"从{filepath}加载模型")
        
        with open(filepath, 'r') as f:
            model_data = json.load(f)
        
        # 创建模型实例
        model = cls(
            grid_size=tuple(model_data['grid_size']),
            input_dim=model_data['input_dim']
        )
        
        # 更新模型属性
        model.learning_rate = model_data['learning_rate']
        model.radius = model_data['radius']
        model.time_constant = model_data['time_constant']
        
        # 重建节点
        model.nodes = {}
        for node_id, node_data in model_data['nodes'].items():
            model.nodes[node_id] = QuantumNode.from_dict(node_data)
            
        return model

# 使用示例
if __name__ == "__main__":
    # 创建模型
    model = SOMModel(grid_size=(10, 10), input_dim=8)
    
    # 生成一些随机数据
    data = [np.random.random(8) for _ in range(100)]
    
    # 训练模型
    model.train(data, iterations=100)
    
    # 测试映射功能
    test_vector = np.random.random(8)
    bmu_id, similarities = model.map_input(test_vector)
    
    print(f"最佳匹配单元: {bmu_id}")
    print(f"前3个最相似节点: {sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:3]}")
    
    # 保存模型
    model.save() 