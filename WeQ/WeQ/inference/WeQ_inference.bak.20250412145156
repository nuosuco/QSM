#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
WeQ推理服务
用于提供WeQ模型的推理功能
开发团队: 中华 ZhoHo, Claude
"""

import os
import sys
import json
import time
import logging
import argparse
import numpy as np
from datetime import datetime
import traceback

# 确保能够导入WeQ模块
script_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(script_dir, ".."))

if project_root not in sys.path:
    sys.path.append(project_root)

# 设置日志
def setup_logging():
    log_dir = os.path.join(project_root, ".logs")
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = os.path.join(log_dir, f"WeQ_inference_{timestamp}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger("WeQ_Inference")

logger = setup_logging()

try:
    # 导入WeQ核心模块
    from WeQ.weq_core import WeQCore
    logger.info("成功导入WeQ核心模块")
except ImportError as e:
    logger.error(f"导入WeQ模块失败: {str(e)}")
    logger.error(traceback.format_exc())
    sys.exit(1)

# 全局变量
weq_core = None
MODEL_CONFIG = {
    "name": "WeQ量子模型",
    "version": "0.9.5",
    "description": "基于量子计算的WeQ推理模型",
}

# 加载WeQ模型
def load_model():
    global weq_core
    try:
        logger.info("正在加载WeQ模型...")
        weq_core = WeQCore()
        logger.info("WeQ模型加载成功")
        return True
    except Exception as e:
        logger.error(f"WeQ模型加载失败: {str(e)}")
        logger.error(traceback.format_exc())
        return False

# 推理函数
def inference(input_data, **options):
    """
    执行WeQ模型推理
    
    参数:
    - input_data: 输入数据，可以是字符串、列表或字典
    - options: 额外的推理选项
      - format: 输出格式，可选值为'raw'、'json'、'text'，默认为'json'
      - threshold: 阈值，默认为0.5
      - normalize: 是否规范化输出，默认为True
    
    返回:
    根据format参数返回相应格式的推理结果
    """
    if weq_core is None:
        raise RuntimeError("模型未加载，无法进行推理")
    
    # 记录开始时间
    start_time = time.time()
    
    # 处理选项参数
    format_type = options.get('format', 'json')
    threshold = float(options.get('threshold', 0.5))
    normalize = bool(options.get('normalize', True))
    
    # 处理输入数据
    try:
        # 预处理输入数据
        if isinstance(input_data, str):
            # 如果是JSON字符串，尝试解析
            try:
                data = json.loads(input_data)
            except json.JSONDecodeError:
                # 不是JSON，作为文本处理
                data = {"text": input_data}
        else:
            # 直接使用输入数据
            data = input_data
        
        # 转换为模型需要的格式
        processed_input = preprocess_input(data)
        
        # 执行模型推理
        logger.info(f"执行推理: 输入大小={sys.getsizeof(processed_input)}")
        raw_output = weq_core.process(processed_input)
        
        # 后处理输出
        result = postprocess_output(raw_output, 
                                   format_type=format_type, 
                                   threshold=threshold,
                                   normalize=normalize)
        
        # 记录处理时间
        process_time = time.time() - start_time
        logger.info(f"推理完成，耗时: {process_time:.4f}秒")
        
        return result
        
    except Exception as e:
        logger.error(f"推理过程中发生错误: {str(e)}")
        logger.error(traceback.format_exc())
        raise

# 预处理输入数据
def preprocess_input(data):
    """
    将输入数据转换为模型可接受的格式
    
    参数:
    - data: 输入数据，可以是字符串、列表或字典
    
    返回:
    处理后的数据
    """
    try:
        if isinstance(data, dict):
            # 从字典中提取相关字段
            if "text" in data:
                # 文本输入
                return {"type": "text", "data": data["text"]}
            elif "vector" in data:
                # 向量输入
                return {"type": "vector", "data": data["vector"]}
            elif "image" in data:
                # 图像输入 (Base64编码)
                return {"type": "image", "data": data["image"]}
            else:
                # 默认作为通用数据处理
                return {"type": "generic", "data": data}
        elif isinstance(data, list):
            # 列表输入，假设为特征向量
            return {"type": "vector", "data": data}
        elif isinstance(data, str):
            # 字符串输入，作为文本处理
            return {"type": "text", "data": data}
        else:
            # 其他类型，转换为字符串
            return {"type": "text", "data": str(data)}
    except Exception as e:
        logger.error(f"预处理输入数据失败: {str(e)}")
        logger.error(traceback.format_exc())
        raise

# 后处理输出数据
def postprocess_output(output, format_type='json', threshold=0.5, normalize=True):
    """
    后处理模型输出
    
    参数:
    - output: 模型原始输出
    - format_type: 输出格式，可选值为'raw'、'json'、'text'
    - threshold: 阈值，用于过滤低分结果
    - normalize: 是否规范化输出
    
    返回:
    处理后的输出
    """
    try:
        # 应用阈值
        if isinstance(output, dict) and "scores" in output:
            filtered_results = []
            for i, score in enumerate(output["scores"]):
                if score >= threshold:
                    filtered_results.append({
                        "index": i,
                        "score": score,
                        "label": output.get("labels", ["未知"])[i] if i < len(output.get("labels", [])) else "未知"
                    })
            
            # 排序结果
            filtered_results.sort(key=lambda x: x["score"], reverse=True)
            
            # 规范化分数
            if normalize and filtered_results:
                total = sum(item["score"] for item in filtered_results)
                if total > 0:
                    for item in filtered_results:
                        item["score"] = item["score"] / total
            
            # 根据format_type返回不同格式
            if format_type == 'raw':
                return output
            elif format_type == 'json':
                return {
                    "results": filtered_results,
                    "count": len(filtered_results),
                    "threshold": threshold
                }
            elif format_type == 'text':
                if not filtered_results:
                    return "无符合阈值的结果"
                
                text_result = "推理结果:\n"
                for i, item in enumerate(filtered_results):
                    text_result += f"{i+1}. {item['label']}: {item['score']:.4f}\n"
                return text_result.strip()
            else:
                # 未知格式，返回JSON
                return {
                    "results": filtered_results,
                    "count": len(filtered_results),
                    "threshold": threshold
                }
        else:
            # 如果输出不是预期格式，直接返回
            return output
    except Exception as e:
        logger.error(f"后处理输出数据失败: {str(e)}")
        logger.error(traceback.format_exc())
        raise

# 批量推理函数
def batch_inference(inputs, **options):
    """
    批量执行WeQ模型推理
    
    参数:
    - inputs: 输入数据列表
    - options: 额外的推理选项
    
    返回:
    推理结果列表
    """
    if not isinstance(inputs, list):
        raise ValueError("批量推理输入必须是列表")
    
    results = []
    for input_data in inputs:
        result = inference(input_data, **options)
        results.append(result)
    
    return results

# 服务主循环
def service_loop():
    """
    推理服务主循环，从指定队列或接口获取任务并处理
    """
    logger.info("推理服务主循环启动")
    
    try:
        # 这里可以实现从消息队列、文件监视或其他源获取任务的逻辑
        # 简化版本：每隔一段时间检查是否有新任务
        
        while True:
            # 检查是否有终止信号
            if os.path.exists(os.path.join(project_root, ".logs", "stop_inference")):
                logger.info("检测到终止信号，服务停止")
                break
            
            # 休眠一段时间
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("接收到键盘中断，服务停止")
    except Exception as e:
        logger.error(f"服务循环中发生错误: {str(e)}")
        logger.error(traceback.format_exc())
    
    logger.info("推理服务主循环结束")

# 主函数
def main():
    parser = argparse.ArgumentParser(description='WeQ推理服务')
    parser.add_argument('--batch', action='store_true', help='启用批处理模式')
    parser.add_argument('--input', type=str, help='输入数据文件路径')
    parser.add_argument('--output', type=str, help='输出结果文件路径')
    parser.add_argument('--format', type=str, default='json', choices=['raw', 'json', 'text'], help='输出格式')
    parser.add_argument('--threshold', type=float, default=0.5, help='阈值')
    parser.add_argument('--service', action='store_true', help='作为服务运行')
    
    args = parser.parse_args()
    
    # 输出启动信息
    logger.info("="*50)
    logger.info(f"启动WeQ推理服务 v{MODEL_CONFIG['version']}")
    logger.info(f"项目根目录: {project_root}")
    logger.info("="*50)
    
    # 加载模型
    if not load_model():
        logger.error("模型加载失败，服务无法启动")
        sys.exit(1)
    
    # 根据运行模式执行不同操作
    if args.service:
        # 作为服务运行
        logger.info("以服务模式运行")
        service_loop()
    elif args.input:
        # 处理输入文件
        try:
            logger.info(f"从文件加载输入: {args.input}")
            with open(args.input, 'r', encoding='utf-8') as f:
                input_data = json.load(f)
            
            # 执行推理
            if args.batch:
                results = batch_inference(input_data, format=args.format, threshold=args.threshold)
            else:
                results = inference(input_data, format=args.format, threshold=args.threshold)
            
            # 输出结果
            if args.output:
                logger.info(f"将结果写入文件: {args.output}")
                with open(args.output, 'w', encoding='utf-8') as f:
                    if args.format == 'text':
                        f.write(results if isinstance(results, str) else json.dumps(results, ensure_ascii=False, indent=2))
                    else:
                        json.dump(results, f, ensure_ascii=False, indent=2)
            else:
                # 打印到控制台
                if args.format == 'text':
                    print(results if isinstance(results, str) else json.dumps(results, ensure_ascii=False, indent=2))
                else:
                    print(json.dumps(results, ensure_ascii=False, indent=2))
                    
        except Exception as e:
            logger.error(f"处理输入文件失败: {str(e)}")
            logger.error(traceback.format_exc())
            sys.exit(1)
    else:
        # 没有提供输入文件，进入服务模式
        logger.info("未提供输入文件，以服务模式运行")
        service_loop()
    
    logger.info("WeQ推理服务已停止")

if __name__ == "__main__":
    main()

"""
量子基因编码: QE-WEQ-INF-10A8F5C3
纠缠状态: 活跃
纠缠对象: ['WeQ/weq_core.py']
纠缠强度: 0.98
"""

# 开发团队：中华 ZhoHo ，Claude 